{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction This is whuwzp's papapa wiki! 主要是仿照慕课大神重写爬虫. Copyright © whuwzp.top 2018 all right reserved，powered by Gitbook本文档修订时间： 2018-07-12 22:12:18 "},"1.html":{"url":"1.html","title":"爬取地图","keywords":"","body":" MOOC 网爬虫项目，照着大神的方法。这个是听课笔记吧。 获取用户 由人工搜索启发： 城市列表 猜你喜欢 用户ID+1 存在的问题 网址发生变化，改成：http://city.zhenai.com/ 爬取内容出现乱码，\"\" gopm 安装go包的方法 golang.org/x/text的下载，https://github.com/golang/text，下载后放进src目录 大神代码 package main import ( \"net/http\" \"fmt\" \"io/ioutil\" \"golang.org/x/text/transform\" \"golang.org/x/net/html/charset\" \"io\" \"golang.org/x/text/encoding\" \"bufio\" ) func main() { resp, err := http.Get( \"http://city.zhenai.com\") if err != nil { panic(err) } defer resp.Body.Close() if resp.StatusCode != http.StatusOK { fmt.Println(\"error! statuscode = \", resp.StatusCode) return } utf8Reader := transform.NewReader(resp.Body, DetermineEncoding(resp.Body).NewDecoder()) all, err := ioutil.ReadAll(utf8Reader) if err != nil { panic(err) } fmt.Printf(\"%s\\n\", all) } func DetermineEncoding(r io.Reader) encoding.Encoding { bytes, err := bufio.NewReader(r).Peek(1024) if err != nil { panic(err) } e, _, _ := charset.DetermineEncoding(bytes, \"\") return e } Copyright © whuwzp.top 2018 all right reserved，powered by Gitbook本文档修订时间： 2018-06-28 22:52:56 "},"2.html":{"url":"2.html","title":"获取城市列表和URL","keywords":"","body":" MOOC 网爬虫项目，照着大神的方法。这个是听课笔记吧。 本节是利用正则表达式找出爬取内容中的城市和URL。 原理 概要 上一节大神讲到了爬取了网页内容，接下来要找出城市列表和url，以便进行下一步人员爬取。主要有以下方法： CSS选择器 xpath 正则表达式 大神选择了正则表达式的方法。 正则表达式 简单示例 package main import ( \"regexp\" \"fmt\" ) const text = `my name is whuwzp@sina.com name2 is whuwzp@163.com name3 is whuwzp@126.com ` func main() { re := regexp.MustCompile(\"whuwzp\") match := re.FindString(text) fmt.Println(match) } //output whuwzp 其中： Mustcompile相比Compile是内嵌了异常报错。 FindString有很多变种，如FindAllStringSubmatch，表示找出字符串中所有子匹配项，返回的是[][]string，变种方法，比如去掉All，去掉String（那就是找出类型为[]byte的），Submatch为自匹配，具体可以利用go doc命令查看各个方法。 匹配特定字符 ``用法：考虑字符中可能存在特殊字符，需要用到转义字符，所以go采用``的方法避免使用过多的\\（转义字符）； .+用法：表示通配任意字符（数量大于0）； .*用法：表示通配任意字符（数量大于等于0）； []用法：表示通配在某区间内的字符，如[0-9a-zA-Z]表示通配数字和大小写字母，[0-9a-zA-Z]+则表示任意个数字和大小写字母； x：表示通配除了符号x外的其他任意字符； ()用法：表示摘取特定的小段字符串，在FindAllStringSubmatch函数中可以体现这个的用处； 项目 上一步爬取了网页内容，这个在内容中找出城市列表和URL等。 整体代码 大神这个是实现了打印出所有的城市和相应的URL。 package main import ( \"net/http\" \"fmt\" \"io/ioutil\" \"golang.org/x/text/transform\" \"golang.org/x/net/html/charset\" \"io\" \"golang.org/x/text/encoding\" \"bufio\" \"regexp\" ) const InitUrl = \"http://city.zhenai.com\" func main() { resp, err := http.Get( InitUrl) if err != nil { panic(err) } defer resp.Body.Close() if resp.StatusCode != http.StatusOK { fmt.Println(\"error! statuscode = \", resp.StatusCode) return } utf8Reader := transform.NewReader(resp.Body, DetermineEncoding(resp.Body).NewDecoder()) all, err := ioutil.ReadAll(utf8Reader) if err != nil { panic(err) } //fmt.Printf(\"%s\\n\", all) PrintCityList(string(all)) } func PrintCityList(all string) { re := regexp.MustCompile(`]*>([^ 下一步 下一步就可以递归爬取URL，继续采用正则表达式，找出人和她对应的信息了。 Copyright © whuwzp.top 2018 all right reserved，powered by Gitbook本文档修订时间： 2018-06-28 22:53:02 "},"3.html":{"url":"3.html","title":"单任务版爬虫","keywords":"","body":" MOOC 网爬虫项目，照着大神的方法。这个是听课笔记吧。 上节已经做好了基本的操作，本节是初步形成单任务版爬虫的基本架构。 项目 目前已完成了人的链接获取，下一步需要对人的信息进行获取（难点在于权限不够，出现403拒绝访问错误） 整体代码 附录 知识点 结构体中含有函数成员 type Request struct { Url string PaserFunc func([]byte) PaserResult } 类似之前的温馨项目，最初我想用接口实现，后来发现这种函数更简单些。调用一样是.PaserFunc。 ...的用法 例如，有个slice A，想把它加到另一个slice B，一般就需要用: for _, a := range A { B = append(B, a) } 这样就比较麻烦，Go提供了简洁方法： B = append(B, A...)//二者等效 另外，在参数上也可以使用相同的方法： func Run(seeds ...deffer.Request) { fmt.Printf(\"seeds: %T\\n\", seeds) var REQS []deffer.Request REQS = append(REQS, seeds...) } //输出 seeds: []deffer.Request //可见，也就是相当于数组类型 //但是，这样的参数允许一个数，而不是数组，作为参数 //它允许deffer.Request一个值，也允许[]deffer.Request的数组 GO Test使用 这里为了测试单个环节采用了Go test功能。以Goland IDE为例： 现在待测试函数目录内新建xxx_test.go文件； 新建TestCityPaser函数，并编写测试函数； import \"testing\" func TestCityPaser(t *testing.T) { } 在Goland中增加go test的运行配置，如图： 问题与思考 思考1-爬死循环 如果请求中存在重复怎么办？如果形成了循环怎么办（没法停下来了）？ 思考2-同人重复URL 需要注意同一个信息的重复，例如，用户，相片 这样的，可以点击用户名和照片链接的，可能出现重复，这时就要筛选了。 思考3-城市筛选 本来是想在MenPaser中实现下一页的，但是如果还按照CityPaser里的筛选的话，就会有其他的城市被选进来（网站中的其他城市链接）。所以必须筛选。有以下的思路： 直接找出下一页的特征，如： 下一页 明显的下一页这个词，或者next-page。 筛去其他的城市，所以先找出本城市，然后增加筛选的长度： href 属性规定被链接文档的位置（URL）。 最终采用第一种： `]*>下一页 因为上一页也是next-page，所以不得不加上下一页的筛选条件。 思考4-网站403 会遇到403禁止访问的问题，这是网站对自动化爬虫的禁止。 error! statuscode = 403 BUG1-unnamed filed initialization Copyright © whuwzp.top 2018 all right reserved，powered by Gitbook本文档修订时间： 2018-06-28 22:53:06 "}}